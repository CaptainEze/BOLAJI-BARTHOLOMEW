{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6f2743",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "import os\n",
    "\n",
    "\n",
    "RESULT_DIR = '../results'\n",
    "FIG_DIR = f'{RESULT_DIR}/figs'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Clean data import phase\n",
    "df = pd.read_csv(\"../data/dataCombined.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e59050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for each section prefix\n",
    "DEMOGRAPHICS = \"D_\"\n",
    "USAGE = \"U_\"\n",
    "ADOPTION = \"A_\"\n",
    "FACTORS = \"F_\"\n",
    "PROJECT_DELIVERY = \"P_\"\n",
    "SUSTAINABILITY = \"S_\"\n",
    "BARRIERS = \"B_\"\n",
    "\n",
    "\n",
    "PLOT_PALATTE = [\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baddaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for categorical data\n",
    "demographic_cols = [col for col in df.columns if col.startswith(DEMOGRAPHICS)]\n",
    "usage_cols = [col for col in df.columns if col.startswith(USAGE)]\n",
    "\n",
    "for col in demographic_cols + usage_cols:\n",
    "    print(f\"\\n--- Frequency Table: {col} ---\")\n",
    "    print(df[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc0ae2",
   "metadata": {},
   "source": [
    "## Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "likert_cols = [col for col in df.columns if col.startswith((ADOPTION))]\n",
    "\n",
    "means = df[likert_cols].mean()\n",
    "medians = df[likert_cols].median()\n",
    "modes = df[likert_cols].mode().iloc[0]\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Mean': means,\n",
    "    'Median': medians,\n",
    "    'Mode': modes\n",
    "})\n",
    "\n",
    "summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed34c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define category prefixes\n",
    "categories = {\n",
    "    \"Demographics\": DEMOGRAPHICS,\n",
    "    \"Usage\": USAGE,\n",
    "    \"Adoption\": ADOPTION,\n",
    "    \"Factors\": FACTORS,\n",
    "    \"Project Delivery\": PROJECT_DELIVERY,\n",
    "    \"Sustainability\": SUSTAINABILITY,\n",
    "    \"Barriers\": BARRIERS\n",
    "}\n",
    "\n",
    "# Count columns in each category\n",
    "category_counts = {}\n",
    "for name, prefix in categories.items():\n",
    "    category_counts[name] = len([col for col in df.columns if col.startswith(prefix)])\n",
    "\n",
    "# Calculate \"Other\" for columns that don't match any category\n",
    "other_cols = len(df.columns) - sum(category_counts.values())\n",
    "if other_cols > 0:\n",
    "    category_counts[\"Other\"] = other_cols\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(\n",
    "    category_counts.values(),\n",
    "    labels=category_counts.keys(),\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=plt.cm.Pastel1.colors\n",
    ")\n",
    "\n",
    "plt.title('Distribution of Columns by Category')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2911af",
   "metadata": {},
   "source": [
    "### Frequency plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2298ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_distribution(\n",
    "    df, \n",
    "    column, \n",
    "    x_labels_dict,\n",
    "    x_label='Category', \n",
    "    y_label='Frequency', \n",
    "    title='Distribution', \n",
    "    filename='output.png',\n",
    "):\n",
    "        \n",
    "    categories_present = sorted(df[column].dropna().unique())\n",
    "\n",
    "    labels = [x_labels_dict[val] for val in categories_present]\n",
    "    palette = sns.color_palette(\"pastel\", n_colors=len(labels))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.countplot(x=column, data=df, order=categories_present, palette=palette)\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, fontsize=10)\n",
    "\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.title(title, fontsize=14, weight='bold')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2., height + 0.5,\n",
    "                int(height), ha=\"center\", fontsize=9)\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{FIG_DIR}/{filename}\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbd033",
   "metadata": {},
   "source": [
    "# Frequency plots for Demographic fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gender_labels_dict = {\n",
    "    1: \"Male\",\n",
    "    2: \"Female\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_Gender\",\n",
    "#     x_labels_dict=gender_labels_dict,\n",
    "#     x_label=\"Gender\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Gender Distribution\",\n",
    "#     filename=\"gender_distribution.png\"\n",
    "# )\n",
    "age_labels_dict = {\n",
    "    1: \"<20\", \n",
    "    2: \"21-30\", \n",
    "    3: \"31-40\", \n",
    "    4: \"41-50\", \n",
    "    5: \"Over 50\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_Age\",\n",
    "#     x_labels_dict=age_labels_dict,\n",
    "#     x_label=\"Age Group\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Age Distribution\",\n",
    "#     filename=\"age_distribution.png\"\n",
    "# )\n",
    "education_labels_dict = {\n",
    "    1: \"OND\",\n",
    "    2: \"B.Sc./HND\",\n",
    "    3: \"PGD\",\n",
    "    4: \"MSc.\",\n",
    "    5: \"Ph.D.\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_Education\",\n",
    "#     x_labels_dict=education_labels_dict,\n",
    "#     x_label=\"Educational Qualification\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Educational Qualification Distribution\",\n",
    "#     filename=\"education_distribution.png\"\n",
    "# )\n",
    "position_labels_dict = {\n",
    "    1: \"Junior Staff\",\n",
    "    2: \"Senior Staff\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_Position\",\n",
    "#     x_labels_dict=position_labels_dict,\n",
    "#     x_label=\"Staff Position\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Position Distribution\",\n",
    "#     filename=\"position_distribution.png\"\n",
    "# )\n",
    "experience_labels_dict = {\n",
    "    1: \"<5 years\",\n",
    "    2: \"6-10 years\",\n",
    "    3: \"11-15 years\",\n",
    "    4: \"16-20 years\",\n",
    "    5: \"Over 20 years\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_Experience\",\n",
    "#     x_labels_dict=experience_labels_dict,\n",
    "#     x_label=\"Years of Experience\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Experience Distribution\",\n",
    "#     filename=\"experience_distribution.png\"\n",
    "# )\n",
    "company_age_labels_dict = {\n",
    "    1: \"1-5 years\",\n",
    "    2: \"6-10 years\",\n",
    "    3: \"11-15 years\",\n",
    "    4: \"16-20 years\",\n",
    "    5: \"Over 20 years\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_CompanyAge\",\n",
    "#     x_labels_dict=company_age_labels_dict,\n",
    "#     x_label=\"Company Age\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Company Age Distribution\",\n",
    "#     filename=\"company_age_distribution.png\"\n",
    "# )\n",
    "employee_count_labels_dict = {\n",
    "    1: \"1-5\",\n",
    "    2: \"6-10\",\n",
    "    3: \"11-15\",\n",
    "    4: \"16-20\",\n",
    "    5: \"Above 20\"\n",
    "}\n",
    "# plot_categorical_distribution(\n",
    "#     df=df,\n",
    "#     column=\"D_NumEmployees\",\n",
    "#     x_labels_dict=employee_count_labels_dict,\n",
    "#     x_label=\"Number of Employees\",\n",
    "#     y_label=\"Number of Respondents\",\n",
    "#     title=\"Employee Count Distribution\",\n",
    "#     filename=\"employee_count_distribution.png\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#Area plot for Usage fields\n",
    "usage_cols = [col for col in df.columns if col.startswith(USAGE)]\n",
    "\n",
    "usage_distribution = pd.DataFrame(index=range(1, 6))\n",
    "\n",
    "for col in usage_cols:\n",
    "    # Get value counts in order 1-5\n",
    "    counts = df[col].value_counts().sort_index()\n",
    "    percentages = (counts / counts.sum() * 100).round(0).astype(int).astype(str) + '%'\n",
    "    \n",
    "    # Combine counts and percentages\n",
    "    combined = counts.astype(str) + ' (' + percentages + ')'\n",
    "    \n",
    "    # Add to our distribution table\n",
    "    usage_distribution[col] = combined\n",
    "\n",
    "usage_distribution = usage_distribution.T\n",
    "\n",
    "# Optionally: Rename the index for clarity\n",
    "usage_distribution.columns = [\n",
    "    'Very Low (1)',\n",
    "    'Low (2)',\n",
    "    'Moderate (3)',\n",
    "    'High (4)',\n",
    "    'Very High (5)'\n",
    "]\n",
    "\n",
    "usage_distribution['Total'] = df[usage_cols].count()\n",
    "\n",
    "print(usage_distribution)\n",
    "\n",
    "# for col in usage_cols:\n",
    "#     counts = df[col].value_counts().reindex(range(1, 6), fill_value=0)\n",
    "#     usage_distribution[col] = counts\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for col in usage_distribution.columns:\n",
    "#     plt.fill_between(usage_distribution.index, usage_distribution[col], alpha=0.15)\n",
    "#     plt.plot(usage_distribution.index, usage_distribution[col], label=col)\n",
    "\n",
    "# plt.title(\"Digital Technology Usage Distribution\", fontsize=14, weight='bold')\n",
    "# plt.xlabel(\"Response Level (5 = Very High ... 1 = Very Low)\", fontsize=12)\n",
    "# plt.ylabel(\"Number of Respondents\", fontsize=12)\n",
    "# plt.xticks([1, 2, 3, 4, 5])\n",
    "# plt.grid(True, linestyle='--', alpha=0.6)\n",
    "# plt.legend(loc='upper right', fontsize=9)\n",
    "# sns.despine()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(f\"{FIG_DIR}/usage_area_chart.png\", dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b4418",
   "metadata": {},
   "source": [
    "# Data Reliability and Validity\n",
    "### Cronbach’s Alpha for Reliability\n",
    "To access internal consistency of sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data reliability and validity\n",
    "import pingouin as pg\n",
    "adoption_cols = [col for col in df.columns if col.startswith(ADOPTION)]\n",
    "project_delivery_cols = [col for col in df.columns if col.startswith(PROJECT_DELIVERY)]\n",
    "sustainability_cols = [col for col in df.columns if col.startswith(SUSTAINABILITY)]\n",
    "barriers_cols = [col for col in df.columns if col.startswith(BARRIERS)]\n",
    "\n",
    "\n",
    "a_alpha, a_ci = pg.cronbach_alpha(data=df[adoption_cols])\n",
    "p_alpha, p_ci = pg.cronbach_alpha(data=df[project_delivery_cols])\n",
    "s_alpha, s_ci = pg.cronbach_alpha(data=df[sustainability_cols])\n",
    "b_alpha, b_ci = pg.cronbach_alpha(data=df[barriers_cols])\n",
    "\n",
    "print(f\"Cronbach's alpha for adoption = {a_alpha:.3f} (95% CI: {a_ci[0]:.3f} - {a_ci[1]:.3f})\")\n",
    "print(f\"Cronbach's alpha for project delivery = {p_alpha:.3f} (95% CI: {p_ci[0]:.3f} - {p_ci[1]:.3f})\")\n",
    "print(f\"Cronbach's alpha for sustainability = {s_alpha:.3f} (95% CI: {s_ci[0]:.3f} - {s_ci[1]:.3f})\")\n",
    "print(f\"Cronbach's alpha for barriers = {b_alpha:.3f} (95% CI: {b_ci[0]:.3f} - {b_ci[1]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3c08f",
   "metadata": {},
   "source": [
    "# Exploratory Factor Analysis (EFA)\n",
    "Exploratory Factor Analysis (EFA) is a key step in identifying underlying latent variables that explain the observed correlations between measured variables. We will use this to interpret the loadings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f089e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471a600",
   "metadata": {},
   "source": [
    "### Factor Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_factor_analysis(df, cols, n_factors=5, rotation='varimax'):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[cols])\n",
    "\n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)\n",
    "    fa.fit(scaled_data)\n",
    "\n",
    "    eigenvalues = fa.get_eigenvalues()\n",
    "    loadings = fa.loadings_\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--')\n",
    "    plt.title(f\"Scree Plot for {', '.join(cols)}\")\n",
    "    plt.xlabel('Factors')\n",
    "    plt.ylabel('Eigenvalue')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    loadings_df = pd.DataFrame(loadings, index=cols)\n",
    "    print(f'Factor Loadings for {\", \".join(cols)}:\\n', loadings_df)\n",
    "\n",
    "    return fa, loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfe49a",
   "metadata": {},
   "source": [
    "### Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fa_adoption, loadings_adoption = run_factor_analysis(df, adoption_cols)\n",
    "# fa_project_delivery, loadings_project_delivery = run_factor_analysis(df, project_delivery_cols)\n",
    "# fa_sustainability, loadings_sustainability = run_factor_analysis(df, sustainability_cols)\n",
    "fa_barriers, loadings_barriers = run_factor_analysis(df, barriers_cols)\n",
    "print(fa_barriers, loadings_barriers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72870be",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "PCA reduces the dimensionality of the data while retaining as much variance as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd526977",
   "metadata": {},
   "source": [
    "### PCA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(df, cols, n_components=5):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[cols])\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    print(pca_result)\n",
    "\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "    plt.title(f\"Explained Variance by Principal Components for {', '.join(cols)}\")\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return pca, explained_variance, pca_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80012d9",
   "metadata": {},
   "source": [
    "### PCA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645779e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_adoption, explained_variance_adoption, pca_result_adoption = run_pca(df, adoption_cols)\n",
    "pca_project_delivery, explained_variance_project_delivery, pca_result_project_delivery = run_pca(df, project_delivery_cols)\n",
    "pca_sustainability, explained_variance_sustainability, pca_result_sustainability = run_pca(df, sustainability_cols)\n",
    "pca_barriers, explained_variance_barriers, pca_result_barriers = run_pca(df, barriers_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee713d",
   "metadata": {},
   "source": [
    "### Factor Loadings Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_factor_loadings(loadings_df, title='Factor Loadings'):\n",
    "    plt.figure(figsize=(10, min(0.5 * len(loadings_df), 12)))\n",
    "    sns.heatmap(loadings_df, annot=True, cmap='coolwarm', center=0, fmt=\".2f\")\n",
    "    plt.title(title, fontsize=14, weight='bold')\n",
    "    plt.xlabel(\"Factors\")\n",
    "    plt.ylabel(\"Items\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_factor_loadings(loadings_adoption, title=\"Adoption - Factor Loadings\")\n",
    "# plot_factor_loadings(loadings_project_delivery, title=\"Project delivery - Factor Loadings\")\n",
    "# plot_factor_loadings(loadings_sustainability, title=\"Sustainability - Factor Loadings\")\n",
    "plot_factor_loadings(loadings_barriers, title=\"Barriers - Factor Loadings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6cd8c",
   "metadata": {},
   "source": [
    "### PCA Biplot for 2D Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_biplot(pca, components, features, labels=None, title='PCA Biplot'):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    plt.scatter(components[:, 0], components[:, 1], alpha=0.5, label='Respondents')\n",
    "    \n",
    "    feature_vectors = pca.components_[:2].T \n",
    "    for i, v in enumerate(feature_vectors):\n",
    "        plt.arrow(0, 0, v[0]*3, v[1]*3, \n",
    "                  color='r', alpha=0.5, head_width=0.05)\n",
    "        plt.text(v[0]*3.2, v[1]*3.2, labels[i] if labels else f\"Var{i+1}\", \n",
    "                 color='black', fontsize=9)\n",
    "\n",
    "    plt.axhline(0, linestyle='--', color='gray', linewidth=0.5)\n",
    "    plt.axvline(0, linestyle='--', color='gray', linewidth=0.5)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(title, fontsize=14, weight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "pca_biplot(\n",
    "    pca=pca_adoption,\n",
    "    components=pca_result_adoption,\n",
    "    features=adoption_cols,\n",
    "    labels=adoption_cols,\n",
    "    title=\"PCA Biplot - Adoption\"\n",
    ")\n",
    "\n",
    "pca_biplot(\n",
    "    pca=pca_project_delivery,\n",
    "    components=pca_result_project_delivery,\n",
    "    features=project_delivery_cols,\n",
    "    labels=project_delivery_cols,\n",
    "    title=\"PCA Biplot - Project delivery\"\n",
    ")\n",
    "\n",
    "pca_biplot(\n",
    "    pca=pca_sustainability,\n",
    "    components=pca_result_sustainability,\n",
    "    features=sustainability_cols,\n",
    "    labels=sustainability_cols,\n",
    "    title=\"PCA Biplot - Sustainability\"\n",
    ")\n",
    "\n",
    "pca_biplot(\n",
    "    pca=pca_barriers,\n",
    "    components=pca_result_barriers,\n",
    "    features=barriers_cols,\n",
    "    labels=barriers_cols,\n",
    "    title=\"PCA Biplot - Barriers\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ff334",
   "metadata": {},
   "source": [
    "# 6 Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3620aa",
   "metadata": {},
   "source": [
    "### 6.1 Correlation Analysis — Adoption, Sustainability, Delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Adoption_Avg\"] = df[adoption_cols].mean(axis=1)\n",
    "df[\"Sustainability_Avg\"] = df[sustainability_cols].mean(axis=1)\n",
    "df[\"Delivery_Avg\"] = df[project_delivery_cols].mean(axis=1)\n",
    "\n",
    "\n",
    "corr_matrix = df[[\"Adoption_Avg\", \"Sustainability_Avg\", \"Delivery_Avg\"]].corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix: Adoption, Sustainability, Delivery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22f8f",
   "metadata": {},
   "source": [
    "### 6.2 Group Comparisons - T-Tests or ANOVA\n",
    "Test if adoption levels differ significantly between:\n",
    "    - staff positions (`D_Position`)\n",
    "    - Years of experience (`D_Experience`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "junior = df[df[\"D_Position\"] == 1][\"Adoption_Avg\"]\n",
    "senior = df[df[\"D_Position\"] == 2][\"Adoption_Avg\"]\n",
    "\n",
    "t_stat, p_val = ttest_ind(junior, senior, equal_var=False)\n",
    "print(f\"T-test (Position): t={t_stat:.3f}, p={p_val:.3f}\")\n",
    "\n",
    "\n",
    "anova = ols('Adoption_Avg ~ C(D_Experience)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(anova, typ=2)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cb77c",
   "metadata": {},
   "source": [
    "### 6.3 Multiple Regression — Predicting Delivery from Adoption\n",
    "Use adoption score to predict project delivery performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = df[[\"Adoption_Avg\"]]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Dependent variable\n",
    "y = df[\"Delivery_Avg\"]\n",
    "\n",
    "# Fit regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# with demographic controls\n",
    "\n",
    "X = df[[\"Adoption_Avg\", \"D_Position\", \"D_Experience\"]]\n",
    "X = sm.add_constant(X)\n",
    "model_with_controls = sm.OLS(y, X).fit()\n",
    "print(model_with_controls.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98ee30",
   "metadata": {},
   "source": [
    "# 7 Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5adfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "from models.classification import classify_adoption_delivery\n",
    "from models.clustering import cluster_respodents\n",
    "from models.decision_tree import decision_tree_adoption\n",
    "from models.evaluate import evaluate_model\n",
    "\n",
    "def run_model_analysis(df):\n",
    "    \"\"\"\n",
    "    Executes all ML model workflows from Phase 7 on the provided DataFrame.\n",
    "    Returns a dictionary of results from classification, clustering, decision tree, and evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # --- 1. Classification: Predict high vs. low delivery ---\n",
    "    print(\"Running classification (Random Forest)...\")\n",
    "    classification_report_data, clf = classify_adoption_delivery(df)\n",
    "    results['classification'] = {\n",
    "        \"model\": clf,\n",
    "        \"report\": classification_report_data\n",
    "    }\n",
    "\n",
    "    # --- 2. Clustering: Segment firms/respondents ---\n",
    "    print(\"Running clustering (KMeans)...\")\n",
    "    df, kmeans = cluster_respodents(df)\n",
    "    cluster_counts = df[\"Cluster\"].value_counts().to_dict()\n",
    "    results['clustering'] = {\n",
    "        \"model\": kmeans,\n",
    "        \"cluster_distribution\": cluster_counts\n",
    "    }\n",
    "\n",
    "    # --- 3. Decision Tree: Identify adoption success rules ---\n",
    "    print(\"Training decision tree model...\")\n",
    "    tree_model, feature_names = decision_tree_adoption(df)\n",
    "    results['decision_tree'] = {\n",
    "        \"model\": tree_model,\n",
    "        \"features\": list(feature_names)\n",
    "    }\n",
    "\n",
    "    # --- 4. Evaluation Metrics for Classification ---\n",
    "    print(\"Calculating evaluation metrics...\")\n",
    "    # Reuse X/y split for prediction\n",
    "    X = df[[\"Adoption_Avg\", \"D_Position\", \"D_Experience\"]]\n",
    "    y_true = (df[\"Delivery_Avg\"] > df[\"Delivery_Avg\"].median()).astype(int)\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    eval_scores = evaluate_model(y_true, y_pred)\n",
    "    results['evaluation'] = eval_scores\n",
    "\n",
    "    print(\"Phase 7 analysis complete.\\n\")\n",
    "    return df, results\n",
    "\n",
    "\n",
    "print(run_model_analysis(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc30c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, confusion_matrix, silhouette_score)\n",
    "import numpy as np\n",
    "\n",
    "# --- 7.1 Classification: Predict high vs. low delivery performance ---\n",
    "def classify_performance(df):\n",
    "    \"\"\"Predict project delivery performance (high/low) based on adoption\"\"\"\n",
    "    # Create target: 1 if delivery above median, else 0\n",
    "    df['Delivery_High'] = (df['Delivery_Avg'] > df['Delivery_Avg'].median()).astype(int)\n",
    "    \n",
    "    # Features: Adoption factors and key demographics\n",
    "    features = ['Adoption_Avg', 'D_Position', 'D_Experience', 'D_Education']\n",
    "    X = df[features]\n",
    "    y = df['Delivery_High']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nClassification Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importances = pd.Series(model.feature_importances_, index=features)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importances.sort_values().plot.barh(color=PLOT_PALATTE[0])\n",
    "    plt.title(\"Feature Importance for Delivery Prediction\", fontsize=14)\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_DIR}/feature_importance_delivery.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, accuracy\n",
    "\n",
    "# --- 7.2 Clustering: Segment firms/respondents ---\n",
    "def cluster_respondents(df):\n",
    "    \"\"\"Cluster respondents based on adoption, delivery, and sustainability\"\"\"\n",
    "    # Prepare clustering features\n",
    "    cluster_features = ['Adoption_Avg', 'Delivery_Avg', 'Sustainability_Avg']\n",
    "    X = df[cluster_features]\n",
    "    \n",
    "    # Find optimal clusters using silhouette score\n",
    "    silhouette_scores = []\n",
    "    cluster_range = range(2, 6)\n",
    "    \n",
    "    for n in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(cluster_range, silhouette_scores, 'bo-')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.title(\"Optimal Cluster Selection\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_DIR}/silhouette_scores.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply best clustering\n",
    "    optimal_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
    "    df['Cluster'] = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(\n",
    "        x='Adoption_Avg', \n",
    "        y='Delivery_Avg', \n",
    "        hue='Cluster', \n",
    "        data=df, \n",
    "        palette=PLOT_PALATTE,\n",
    "        s=100,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title(f\"Respondent Clusters (k={optimal_clusters})\", fontsize=14)\n",
    "    plt.xlabel(\"Adoption Level\")\n",
    "    plt.ylabel(\"Delivery Performance\")\n",
    "    plt.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_DIR}/respondent_clusters.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Cluster profiles\n",
    "    cluster_profiles = df.groupby('Cluster')[cluster_features].mean()\n",
    "    print(\"\\nCluster Profiles:\")\n",
    "    print(cluster_profiles)\n",
    "    \n",
    "    return df, kmeans\n",
    "\n",
    "# --- 7.3 Decision Tree: Identify adoption success rules ---\n",
    "def adoption_success_rules(df):\n",
    "    \"\"\"Identify decision rules for successful technology adoption\"\"\"\n",
    "    # Create target: 1 if adoption above median\n",
    "    df['Adoption_High'] = (df['Adoption_Avg'] > df['Adoption_Avg'].median()).astype(int)\n",
    "    \n",
    "    # Features: Barriers and demographics\n",
    "    barrier_features = [col for col in barriers_cols]\n",
    "    features = barrier_features + ['D_Position', 'D_Experience', 'D_CompanyAge']\n",
    "    X = df[features]\n",
    "    y = df['Adoption_High']\n",
    "    \n",
    "    # Train decision tree\n",
    "    tree = DecisionTreeClassifier(\n",
    "        max_depth=3,  # Limit depth for interpretability\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    tree.fit(X, y)\n",
    "    \n",
    "    # Visualize tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(\n",
    "        tree, \n",
    "        feature_names=features, \n",
    "        class_names=['Low Adoption', 'High Adoption'],\n",
    "        filled=True,\n",
    "        proportion=True,\n",
    "        rounded=True,\n",
    "        fontsize=10\n",
    "    )\n",
    "    plt.title(\"Decision Tree for Adoption Success\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_DIR}/adoption_decision_tree.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Extract rules\n",
    "    tree_rules = export_text(tree, feature_names=features)\n",
    "    print(\"\\nDecision Tree Rules:\")\n",
    "    print(tree_rules)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# --- 7.4 Evaluate Models ---\n",
    "def evaluate_classification(model, X_test, y_test):\n",
    "    \"\"\"Evaluate classification model performance\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIG_DIR}/confusion_matrix.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# --- Execute all ML workflows ---\n",
    "def run_ml_pipeline(df):\n",
    "    print(\"===== MACHINE LEARNING ANALYSIS =====\")\n",
    "    \n",
    "    # 7.1 Classification\n",
    "    print(\"\\nRunning performance classification...\")\n",
    "    clf_model, clf_accuracy = classify_performance(df)\n",
    "    \n",
    "    # 7.2 Clustering\n",
    "    print(\"\\nClustering respondents...\")\n",
    "    df, cluster_model = cluster_respondents(df)\n",
    "    \n",
    "    # 7.3 Decision Tree\n",
    "    print(\"\\nExtracting adoption rules...\")\n",
    "    tree_model = adoption_success_rules(df)\n",
    "    \n",
    "    # 7.4 Evaluation (using classification model)\n",
    "    print(\"\\nEvaluating classification model...\")\n",
    "    # Prepare test data for evaluation\n",
    "    features = ['Adoption_Avg', 'D_Position', 'D_Experience', 'D_Education']\n",
    "    X = df[features]\n",
    "    y = (df['Delivery_Avg'] > df['Delivery_Avg'].median()).astype(int)\n",
    "    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    metrics = evaluate_classification(clf_model, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nMachine Learning Pipeline Complete!\")\n",
    "    return df, {\n",
    "        'classification_model': clf_model,\n",
    "        'clustering_model': cluster_model,\n",
    "        'decision_tree': tree_model,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "\n",
    "# Execute the full pipeline\n",
    "df, ml_results = run_ml_pipeline(df)\n",
    "\n",
    "print(ml_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
